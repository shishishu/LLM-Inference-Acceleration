# Multi-matrix Factorization Attention

## Core Idea

## Core Image
![Figure 2](fig.2.jpg)

![Table 1](table.1.jpg)

## Useful Extensions
[微信] [阶跃公开了自家新型注意力机制：KV缓存消耗直降93.7%，性能不减反增](https://mp.weixin.qq.com/s/q1HCHpzT665BeL54dNVTsA)